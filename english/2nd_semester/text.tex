\newpage
\chapter*{Оригинал}
\addcontentsline{toc}{chapter}{Оригинал}
\vspace{-1ex}
\begin{center}
  Using the Triangle Inequality to Accelerate k-Means by Charles Elkan
\end{center}

\vspace{-1.5em}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
The k-means algorithm is by far the most widely used method for discovering clusters in data. We show how to accelerate it dramatically, while still always computing exactly the same result as the standard algorithm. The accelerated algo\-rithm avoids unnecessary distance calculations by applying the triangle inequality in two different ways, and by keeping track of lower and upper bounds for distances between points and centers. Experiments show that the new algorithm is effective for datasets with up to 1000 dimensions, and becomes more and more effective as the number k of clusters increases. For \( k \ge 20 \) it is many times faster than the best previously known accelerated k-means method.

\chapter{Introduction}
The most common method for finding clusters in data used in applications is the algorithm known as k-means. K-means is considered a fast method because it is not based on computing the distances between all pairs of data points. However, the algorithm is still slow in practice for large datasets. The number of distance computations is \( nke \) where \( n \) is the number of data points, \( k \) is the number of
clusters to be found, and \( e \) is the number of iterations required. Empirically, \( e \) grows sublinearly with \( k \), \( n \), and the dimensionality \( d \) of the data.

The main contribution of this paper is an optimized version of the standard k-means method, with which the number of distance computations is in practice closer to \( n \) than to \( nke \).

The optimized algorithm is based on the fact that most distance calculations in standard k-means are redundant. If a point is far away from a center, it is not necessary to calculate the exact distance between the point and the center in order to know that the point should not be assigned to this center. Conversely, if a point is much closer to one center than to any other, calculating exact distances is not necessary to know that the point should be assigned to the first center. We show below how to make these intuitions concrete.

We want the accelerated k-means algorithm to be usable wherever the standard algorithm is used. Therefore, we need the accelerated algorithm to satisfy three properties. First, it should be able to start with any initial centers, so that all existing initialization methods can continue to be used. Second, given the same initial centers, it should always produce exactly the same final centers as the standard algorithm. Third, it should be able to use any black-box distance metric, so it should not rely for example on optimizations specific to Euclidean distance.

Our algorithm in fact satisfies a condition stronger than the second one above: after each iteration, it produces the same set of center locations as the standard k-means method. This stronger property means that heuristics for merging or splitting centers (and for dealing with empty clusters) can be used together with the new algorithm. The third condition is important because many applications use a domain-specific distance metric. For example, clustering to identify duplicate alphanumeric records is sometimes based on alphanumeric edit distance (Monge \& Elkan, 1996), while clustering of protein structures is often based on an expensive distance function that first rotates and translates structures to superimpose them. Even without a domain-specific metric, recent work shows that using a non-traditional \( L_p \) norm with \( 0 < p < 1 \) is beneficial when clustering in a high-dimensional space (Aggarwal et al., 2001).

\chapter{Applying the triangle inequality}
Our approach to accelerating k-means is based on the triangle inequality: for any three points \( x \), \( y \), and \( z \), \( d(x, z) \le d(x, y) + d(y, z) \). This is the only ``black box'' property that all distance metrics \( d \) possess.

The difficulty is that the triangle inequality gives upper bounds, but we need lower bounds to avoid calculations. Let \( x \) be a point and let \( b \) and \( c \) be centers; we need to know that \( d(x, c) \ge d(x, b) \) in order to avoid calculating the actual value of \( d(x, c) \).

The following two lemmas show how to use the triangle inequality to obtain useful lower bounds.

\textbf{Lemma 1:} Let \( x \) be a point and let \( b \) and \( c \) be centers. If \( d(b, c) \ge 2d(x, b) \) then \( d(x, c) \ge (x, b) \).

\textbf{Proof:} We know that \( d(b, c) \le d(b, x) + d(x, c) \). So \( d(b, c) - d(x, b) \le d(x, c) \). Consider the left-hand side: \( d(b, c) - d(x, b) \ge 2d(x, b) - d(x, b) = d(x, b) \). So \( d(x, b) \le d(x, c) \).

\textbf{Lemma 2:} Let \( x \) be a point and let \( b \) and \( c \) be centers. Then  \( d(x, c) \ge \max\left\{0, d(x, b) - d(b, c)\right\} \).

\textbf{Proof:} We know that \( d(x, b) \le d(x, c) + d(b, c) \), so \( d(x, c) \ge d(x, b) - d(b, c) \). Also, \( d(x, c) \ge 0 \).

Note that Lemmas 1 and 2 are true for any three points, not just for a point and two centers, and the statement of Lemma 2 can be strengthened in various ways.

We use Lemma 1 as follows. Let \( x \) be any data point, let \( c \) be the center to which is currently assigned, and let \( c' \) be any other center. The lemma says that if \( \frac{1}{2} d(c, c') \ge d(x, c) \), then \( d(x, c') \ge d(x, c) \). In this case, it is not necessary to calculate \( d(x, c') \).

Suppose that we do not know \( d(x, c) \) exactly, but we do know an upper bound \( u \) such that \( u \ge d(x, c) \). Then we need to compute \( d(x, c') \) and \( d(x, c) \) only if \( u > \frac{1}{2} d(c, c') \).

If \( u \le \frac{1}{2} \min\left[d(c, c')\right] \) where the minimum is over all \( c' \ne c \), then the point \( x \) must remain assigned to the center \( c \), and all distance calculations for \( x \) can be avoided.

Lemma 2 is applied as follows. Let \( x \) be any data point, let \( b \) be any center, and let \( b' \) be the previous version of the same center. (That is, suppose the centers are numbered \( 1 \) through \( k \), and \( b \) is center number \( j \); then  \( b' \) is center number \( j \) in the previous iteration.) Suppose that in the previous iteration we knew a lower bound \( l' \) such that \( d(x, b') \ge l' \). Then we can infer a lower bound \( l \) for the current iteration:
\[
  d(x, b) \ge \max\left\{0, d(x, b') - d(b, b')\right\} \ge \max\left\{0, l' - d(b, b')\right\} = l.
\]

Informally, if \( l' \) is a good approximation to the previous distance between \( x \) and the \( j \)th center, and this center has moved only a small distance, then \( l \) is a good approximation to the updated distance.

The algorithm below is the first k-means variant that uses lower bounds, as far as we know. It is also the first algorithm that carries over varying information from one k-means iteration to the next. According to the authors of (Kanungo et al., 2000): ``The most obvious source of inefficiency in [our] algorithm is that it passes no information from one stage to the next. Presumably in the later stages of Lloyd's algorithm, as the centers are converging to their final positions, one would expect that the vast majority of the data points have the same closest center from one stage to the next. A good algorithm would exploit this coherence to improve running time.'' The algorithm in this paper achieves this goal. One previous algorithm also reuses information from one k-means iteration in the next, but that method, due to (Judd et al., 1998), does not carry over lower or upper bounds.

Suppose \( u(x) \ge d(x, c) \) is an upper bound on the distance between \( x \) and the center \( c \) to which \( x \) is currently assigned, and suppose \( l(x, c') \le d(x, c') \) is a lower bound on the distance between \( x \) and some other center \( c' \). If \( u(x) \le l(x, c') \) then \( d(x, c) \le u(x) \le l(x, c') \le d(x, c') \) so it is necessary to calculate neither \( d(x, c) \) nor \( d(x, c') \). Note that it will never be necessary in this iteration of the accelerated method to compute \( d(x, c') \), but it may be necessary to compute \( d(x, c) \) exactly because of some other center \( c'' \) for which \( u(x) \le l(x, c'') \) is not true.

\chapter{The new algorithm}
Putting the observations above together, the accelerated k-means algorithm is as follows.

First, pick initial centers. Set the lower bound \( l(x, c) = 0 \) for each point \( x \) and center \( c \). Assign each \( x \) to its closest initial center \( c(x) = \mathrm{argmin}_c \left[d(x, c)\right] \), using Lemma 1 to avoid redundant distance calculations. Each time \( d(x, c) \) is computed, set \( l(x, c) = d(x, c) \). Assign upper bounds \( u(x) = \min_c \left[d(x, c)\right] \).

Next, repeat until convergence:
\begin{enumerate} \itemsep-2pt
  \item For all centers \( c \) and \( c' \), compute \( d(c, c') \). For all centers \( c \), compute \( s(c) = \frac{1}{2} \min_{c'\ne c}d(c, c') \).
  \item Identify all points \( x \) such that \( u(x) \le s(c(x)) \).
  \item For all remaining points \( x \) and centers \( c \) such that
     \begin{enumerate} \itemsep-1.5pt
       \item[(i)] \( c \ne c(x) \) and
       \item[(ii)] \( u(x) > l(x, c) \) and
       \item[(iii)] \( u(x) > \frac{1}{2} d(c(x), c) \):
     \end{enumerate}
     \begin{enumerate} \itemsep-1.5pt
       \item[a] If \( r(x) \) then compute \( d(x, c(x)) \) and assign \( r(x) = \false \). Otherwise, \( d(x, c(x)) = u(x) \).
       \item[b] If \( d(x, c(x)) > l(x, c) \) or \( d(x, c(x)) > \frac{1}{2} d(c(x), c) \) then
         \begin{itemize} \itemsep-1.5pt
           \item[ ] Compute \( d(x, c) \)
           \item[ ] If \( d(x, c) < d(x, c(x)) \) then assign \( c(x) = c \).
         \end{itemize}
     \end{enumerate}
  \item For each center \( c \), let \( m(c) \) be the mean of the points assigned to \( c \).

  \item For each point \( x \) and center \( c \), assign \( l(x, c) = \max\left\{l(x, c) - d(c, m(c)), 0\right\} \).

  \item For each point \( x \), assign \( u(x) = u(x) + d(m(c(x)), c(x)) \) and \( r(x) = \true \).

  \item Replace each center \( c \) by \( m(c) \).
\end{enumerate}

In step (3), each time \( d(x, c) \) is calculated for any \( x \) and \( c \), its lower bound is updated by assigning \( l(x, c) = d(x, c) \). Similarly, \( u(x) \) is updated whenever \( c(x) \) is changed or \( d(x, c(x)) \) is computed. In step (3a), if \( r(x) \) is \( \true \) then \( u(x) \) is out-of-date, i.e. it is possible that \( u(x) \ne d(x, c(x)) \). Otherwise, computing \( d(x, c(x)) \) is not necessary. Step (3b) repeats the checks from (ii) and (iii) in order to avoid computing \( d(x, c) \) if possible.

The fundamental reason why the algorithm above is effective in reducing the number of distance calculations is that at the start of each iteration, the upper bounds \( u(x) \) and the lower bounds \( l(x, c) \) are tight for most points \( x \) and centers \( c \). If these bounds are tight at the start of one iteration, the updated bounds tend to be tight at the start of the next iteration, because the location of most centers changes only slightly, and hence the bounds change only slightly.

The initialization step of the algorithm assigns each point to its closest center immediately. This requires relatively many distance calculations, but it leads to exact upper bounds \( u(x) \) for all \( x \) and to exact lower bounds \( l(x, c) \) for many \( (x, c) \) pairs. An alternative initialization method is to start with each point arbitrarily assigned to one center. The initial values of \( u(x) \) and \( l(c, x) \) are then based on distances calculated to this center only. With this approach, the initial number of distance calculations is only \( n \), but \( u(x) \) and \( l(c, x) \) are much less tight initially, so more distance calculations are required later. (After each iteration each point is always assigned correctly to its closest center, regardless of how inaccurate the lower and upper bounds are at the start of the iteration.) Informal experiments suggest that both initialization methods lead to about the same total number of distance calculations.

Logically, step (2) is redundant because its effect is achieved by condition (iii). Computationally, step (2) is beneficial because if it eliminates a point \( x \) from further consideration, then comparing \( u(x) \) to \( l(x, c) \) for every \( c \) separately is not necessary. Condition (iii) inside step (3) is beneficial despite step (2), because \( u(x) \) and \( c(x) \) may change during the execution of step (3).

We have implemented the algorithm above in Matlab. When step (3) is imp\-le\-men\-ted with nested loops, the outer loop can be over \( x \) or over \( c \). For efficiency in Matlab and similar languages, the outer loop should be over \( c \) since \( k \ll n \) typically, and the inner loop should be replaced by vectorized code that operates on all relevant \( x \) collectively.

Step (4) computes the new location of each cluster center \( c \). Setting \( m(c) \) to be the mean of the points assigned to \( c \) is appropriate when the distance metric in use is Euclidean distance. Otherwise, \( m(c) \) may be defined differently. For example, with k-medians the new center of each cluster is a representative member of the cluster.

\chapter{Experimental results}

This section reports the results of running the new algorithm on six large datasets, five of which are high-dimensional. The datasets are described in Table 1, while Table 2 gives the results.

Our experimental design is similar to the design of (Moore, 2000), which is the best recent paper on speeding up the k-means algorithm for high-dimensional data. However, there is only one dataset used in (Moore, 2000) for which the raw data are available and enough information is given to allow the dataset to be reconstructed. This dataset is called ``covtype.'' Therefore, we also use five other publicly available datasets. None of the datasets have missing data.

In order to make our results easier to reproduce, we use a fixed initialization for each dataset \( X \). The first center is initialized to be the mean of \( X \). Subsequent centers are initialized according to the ``furthest first'' heuristic: each new center is \( \mathrm{argmax}_{x \in X} \min_{c \in C} d(x, c) \) where \( C \) is the set of initial centers chosen so far (Dasgupta, 2002).

Following the practice of past research, we measure the performance of an algorithm on a dataset as the number of distance calculations required. All algo\-rithms that accelerate k-means incur overhead to create and update auxiliary data structures. This means that speedup compared to k-means is always less in clock time than in number of distance calculations. Our algorithm reduces the number of distance calculations so dramatically that its overhead time is often greater than the time spent on distance calculations. However, the total execution time is always much less than the time required by standard k-means. The overhead of the \( l \) and \( u \) data structures will be much smaller with a C implementation than with the Matlab implementation used for the experiments reported here. For this reason, clock times are not reported.

Perhaps the most striking observation to be made from Table 2 is that the relative advantage of the new method increases with \( k \). The number of distance calculations grows only slowly with \( k \) and with \( e \) (the number of passes over the data, called ``iterations'' in Table 2). So much redundant computation is eliminated that the total number of distance calculations is closer to \( n \) than to \( nke \) as for standard k-means.

A related observation is that for \( k \ge 20 \) we obtain a much better speedup than with the anchors method (Moore, 2000). The speedups reported by Moore for the ``covtype'' dataset are 24.8, 11.3, and 19.0 respectively for clustering with 3, 20, and 100 centers. The speedups we obtain are 8.60, 107, and 310. We conjecture that the improved speedup for \( k \ge 20 \) arises in part from using the actual cluster centers as adaptive ``anchors,'' instead of using a set of anchors fixed in preprocessing. The worse speedup for \( k = 3 \) remains to be explained.

Another striking observation is that the new method remains effective even for data with very high dimensionality. Moore writes ``If there is no underlying structure in the data (e.g. if it is uniformly distributed) there will be little or no acceleration in high dimensions no matter what we do. This gloomy view, supported by recent theoretical work in computational geometry (Indyk et al., 1999), means that we can only accelerate datasets that have interesting internal structure.'' While this claim is almost certainly true asymptotically as the dimension of a dataset tends to infinity, our results on the ``random'' dataset suggest that worthwhile speedup can still be obtained up to at least 1000 dimensions. As expected, the more clustered a dataset is, the greater the speedup obtained. Random projection makes clusters more Gaussian (Dasgupta, 2000), so speedup is better for the ``mnist50'' dataset than for the ``mnist784'' dataset.

\begin{table}
  \footnotesize\centering
  \begin{tabular}{|l|r|r|l|} \hline
    name & cardinality & dimensionality & description \\ \hline
    birch & 100000 & 2 & 10 by 10 grid of Gaussian clusters, DS1 in (Zhang et al., 1996) \\ \hline
    covtype & 150000 & 54 & remote soil cover measurements, after (Moore, 2000) \\ \hline
    kddcup & 95413 & 56 & KDD Cup 1998 data, un-normalized \\ \hline
    mnist50 & 60000 & 50 & random projection of NIST handwritten digit training data \\ \hline
    mnist784 & 60000 & 784 & original NIST handwritten digit training data \\ \hline
    random & 10000 & 1000 & uniform random data \\ \hline
  \end{tabular}
  \caption*{\footnotesize \textit{Table 1.} Datasets used in experiments.}
  
  \vspace{2em}
  
  \begin{tabular}{|llrrr|} \hline
             &            & \( k = 3 \) & \( k = 20 \) & \( k = 100 \) \\ \hline
    birch    & iterations &          17 &           38 &            56 \\
             & standard   &   5.100e+06 &    7.600e+07 &     5.600e+08 \\
             & fast       &   4.495e+05 &    1.085e+06 &     1.597e+06 \\
             & speedup    &        11.3 &         70.0 &           351 \\ \hline
    covtype  & iterations &          18 &          256 &           152 \\
             & standard   &   8.100e+06 &    7.680e+08 &     2.280e+09 \\
             & fast       &   9.416e+05 &    7.147e+06 &     7.353e+06 \\
             & speedup    &        8.60 &          107 &           310 \\ \hline
    kddcup   & iterations &          34 &          100 &           325 \\
             & standard   &   9.732e+06 &    1.908e+08 &     3.101e+09 \\
             & fast       &   6.179e+05 &    3.812e+06 &     1.005e+07 \\
             & speedup    &        15.4 &         50.1 &           309 \\ \hline
    mnist50  & iterations &          38 &          178 &           217 \\
             & standard   &   6.840e+06 &    2.136e+08 &     1.302e+09 \\
             & fast       &   1.573e+06 &    9.353e+06 &     3.159e+07 \\
             & speedup    &        4.35 &         22.8 &          41.2 \\ \hline
    mnist784 & iterations &          63 &           60 &           165 \\
             & standard   &   1.134e+07 &    7.200e+07 &     9.900e+08 \\
             & fast       &   1.625e+06 &    7.396e+06 &     3.055e+07 \\
             & speedup    &        6.98 &         9.73 &          32.4 \\ \hline
    random   & iterations &          52 &           33 &            18 \\
             & standard   &   1.560e+06 &    6.600e+06 &     1.800e+07 \\
             & fast       &   1.040e+06 &    3.020e+06 &     5.348e+06 \\
             & speedup    &        1.50 &         2.19 &          3.37 \\ \hline
  \end{tabular}
  \caption*{\footnotesize \textit{Table 2.} Rows labeled ``standard'' and ``fast'' give the number of distance calculations performed by the unaccelerated k-means algorithm and by the new algorithm. Rows labeled ``speedup'' show how many times faster the new algorithm is, when the unit of measurement is distance calculations.}
\end{table}
