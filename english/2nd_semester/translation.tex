\newpage
\setcounter{chapter}{0}
\chapter*{Перевод}
\addcontentsline{toc}{chapter}{Перевод}
\vspace{-1ex}
\begin{center}
  Использование неравенства треугольника для ускорения работы алгоритма k-средних. Автор: Чарльз Элкан.
\end{center}

\vspace{-1.5em}
\chapter*{Реферат}
\addcontentsline{toc}{chapter}{Реферат}
Алгоритм k-средних является наиболее широко используемым методом для обнаружения кластеров в данных. Мы покажем, как кардинально ускорить его работу, всегда получая при этом тот же результат, что и при использовании стандартного алгоритма. Ускоренный алгоритм избегает ненужных вычислений расстояний, применяя неравенство треугольника двумя разными способами и отслеживая нижние и верхние границы расстояний между точками и центрами кластеров. Эксперименты показывают, что новый алгоритм эффективен для наборов данных размерностью вплоть до 1000 измерений, и что он становится всё более эффективным при увеличении числа кластеров \( k \). Для \( k \ge 20 \) новый алгоритм работает во много раз быстрее, нежели самый лучший из известных способов ускорения метода k-средних.

\chapter{Введение}
Наиболее распространенным методом для поиска кластеров в данных, используемых в приложениях, является метод k-средних. Метод k-средних считается быстрым, поскольку он не основан на вычислении расстояний между всеми парами точек. Тем не менее, алгоритм по-прежнему является медленным на практике работы с большими объемами данных. Количество дистанционных расчетов равно \( nke \), где \( n \)~-- это число точек, \( k \)~-- число кластеров и \( e \)~-- число требуемых итераций для схождения метода. Эмпирически, число итерациий растет сублинейно с \( k \), \( n \) и размерностью \( d \) данных.

Основным вкладом данной работы является оптимизированная версия стандартного метода k-средних, в котором число дистанционных расчетов на практике ближе к \( n \), чем к \( nke \).

Оптимизированный алгоритм основан на том факте, что большинство дистанционных измерений в стандартном методе k-средних является лишним. Если точка находится далеко от центра кластера, то и не нужно считать точное расстояние между точкой и центром для того, чтобы узнать, что точка не принадлежит этому кластеру. И наоборот, если точка находится гораздо ближе к одному центру, чем к любому другому, то не нужно считать точные расстояния от точки до остальных центров, чтобы узнать, что точка принадлежит ближнему кластеру. Далее мы покажем, как сделать эти интуитивные высказывания более конкретными.

Мы хотим, чтобы ускоренным метод k-средних мог быть использован в любых приложениях, в которых может использоваться стандартный алгоритм. Таким образом, необходимо, чтобы ускоренный алгоритм удовлетворял трем свойствам. Во-первых, он должен работать с любыми начальными положениями центров, так что все существующие методы инициализации могут использоваться для работы с ускоренным алгоритмом. Во-вторых, при подаче одинаковых данных на входы нового алгоритма и стандартного алгоритма, на выходах мы должны получить одинаковые результаты работы. В-третьих, должна быть возможность использовать любую метрику расстояний, так что алгоритм не должен быть приспособлен для работы с конкретной метрикой, например, эвклидовой.

Наш алгоритм, по факту, удовлетворяет второму требованию несколько сильнее: после каждой итерации он выдает те же центры кластеров, что и стандартный алгоритм. Это свойство означает, что эвристики для слияния или разделения кластеров (и для работы с пустыми кластерами) могут использоваться вместе с новым алгоритмом. Третье условие является очень важным, поскольку множество приложений используют специфичную для конкретной предметной области метрику. Например, кластеризация для определения дубликатов буквенно-цифровых записей иногда основано на расстоянии буквенно-цифровых преобразований (Монж и Элкан, 1996), в то время как кластеризация белковых структур часто основано на сложной функции расстояния, которая сначала поворачивает и транслирует структуру молекулы, чтобы потом наложить их. Даже без специфичной метрики, недавние работы показывают, что использование нетрадиционных \( L_p \) норм с \( 0 < p < 1 \) является полезным, когда кластеризация идет в многомерном пространстве (Аггарваль и др., 2001).

\chapter{Применение неравенства треугольника}
Наш подход к укорению метода k-средних основан на неравенстве треугольника: для любых трех точек \( x \), \( y \) и \( z \) длина одной стороны треугольника меньше или равна сумме двух других: \( d(x, z) \le d(x, y) + d(y, z) \). Это единственное свойство, которым обладают абсолютно все метрики \( d \).

Сложность состоит в том, что неравенство треугольника дает верние границы, а для избавления от ненужных вычислений нам необходимы нижние границы. Пусть \( x \) является точкой, а \( b \) и \( c \)~-- центрами; тогда нам нужно знать, что \( d(x, c) \ge d(x, b) \), для ухода от расчета фактического значения \( d(x, c) \).

Следующие две леммы покажут, как использовать неравенство треугольника для получения полезных нижних границ.

\textbf{Лемма 1:} Пусть \( x \) является точкой, а \( b \) и \( c \)~-- центрами. Если \( d(b, c) \ge 2d(x, b) \), то \( d(x, c) \ge (x, b) \).

\textbf{Доказательство:} Мы знаем, что \( d(b, c) \le d(b, x) + d(x, c) \). Так что \( d(b, c) - d(x, b) \le d(x, c) \). Применим условие леммы в левой части неравенства: \( d(b, c) - d(x, b) \ge 2d(x, b) - d(x, b) = d(x, b) \). Таким образом, \( d(x, b) \le d(x, c) \), что и требовалось доказать.

\textbf{Лемма 2:} Пусть \( x \) является точкой, а \( b \) и \( c \)~-- центрами. Then  \( d(x, c) \ge \max\left\{0, d(x, b) - d(b, c)\right\} \).

\textbf{Доказательство:} Мы знаем, что \( d(x, b) \le d(x, c) + d(b, c) \), поэтому \( d(x, c) \ge d(x, b) - d(b, c) \). Также, \( d(x, c) \ge 0 \).

Заметим, что леммы 1 и 2 справедливы для любых трех точек, а не только для точки и двух центров, и то, что утверждение леммы 2 может быть усилено различными способами.

Мы используем лемму 1 следующим образом. Пусть \( x \) является любой точкой из набора данных, пусть \( c \) является центром кластера, к которому в текущий момент принадлежит точка, и пусть \( c' \) является центром любого другого кластера. Из лемма следует, что если \( \frac{1}{2} d(c, c') \ge d(x, c) \), то \( d(x, c') \ge d(x, c) \). В этом случае, нам не нужно вычислять \( d(x, c') \).

Предположим, что мы точно не знаем \( d(x, c) \), но знаем верхнюю границу \( u \) такую, что \( u \ge d(x, c) \). Тогда нам нужно вычислять \( d(x, c') \) и \( d(x, c) \) только в том случае, если \( u > \frac{1}{2} d(c, c') \).

Если \( u \le \frac{1}{2} \min\left[d(c, c')\right] \), где минимум берется по всем центрам \( c' \ne c \), то точка \( x \) должна остаться принадлежащей кластеру \( c \), а все остальные расчеты расстояний для точки \( x \) могут быть пропущены.

Лемма 2 применяется следующим образом. Пусть \( x \) является любой точкой из набора данных, пусть \( b \) является центром любого кластера, и пусть \( b' \) является предыдущей версией центра того же кластера. (То есть, предпологая что все кластеры пронумерованы от \( 1 \) до \( k \), а \( b \)~-- это кластер с номером \( j \); то \( b' \)~-- это кластер номер \( j \) на предыдущей итерации алгоритма.) Предположим, что на прошлой итерации алгоритма была известна нижняя граница \( l' \) такая, что \( d(x, b') \ge l' \). Тогда можно сделать вывод, что для нижней границы \( l \) на текущей итерации будет справедливо неравенство:
\[
  d(x, b) \ge \max\left\{0, d(x, b') - d(b, b')\right\} \ge \max\left\{0, l' - d(b, b')\right\} = l.
\]

Вообще, если \( l' \) является хорошим приближением к предыдущему расстоянию между \( x \) и \( j \)тым кластером, и этот центр сдвинулся только на небольшое расстояние, то \( l \) является хорошим приближением к обновленной дистанции.

Новый алгоритм является, насколько известно, первым вариантом метода k-средних, в котором используются нижние границы. Это также первый алгоритм, который переносит различную информацию от одной итерации к следующей. По мнению авторов (Канунго и др., 2000): <<самый очевидный источник неэффективности [нашего] алгоритма~-- это то, что он не переносит информации от одного этапа к следующему. Предположительно, на поздних стадиях алгоритма Ллойда, таких как схождение центров к их конечным положениям, можно было бы ожидать, что подавляющее большинство точек имеет один и тот же ближайший центр кластера при переходе на следующую итерацию. В хорошем алгоритме эта согласованность будет использована для улучшения его времени работы.>> Алгоритм в данной статье достигает этой цели. Один из известных алгоритмов также переносит информацию от одной итерации метода k-средних к следующей, но тот алгоритм, в силу (Джудд и др., 1998), не переносит нижние или верхние границы.

Предположим, что \( u(x) \ge d(x, c) \) является верхней границей расстояния между точкой \( x \) и центром кластера \( c \), которому точк \( x \) принадлежит на текущей итерации. Предположим, что \( l(x, c') \le d(x, c') \) является нижней границей расстояния между точкой \( x \) и центром какого-либо другого кластера \( c' \). Если \( u(x) \le l(x, c') \), то \( d(x, c) \le u(x) \le l(x, c') \le d(x, c') \), и тогда нет необходимости вычислять ни \( d(x, c) \), ни \( d(x, c') \). Заметим, что на текущей итерации никогда не будет необходимости в вычислении \( d(x, c') \), но может возникнуть необходимость в вычислении \( d(x, c) \), но только из-за того, что для какого-то другого кластера \( c'' \) неравенство \( u(x) \le l(x, c'') \) не является верным.

\chapter{Новый алгоритм}
Сведя все приведенные выше наблюдения воедино, можно записать ускоренный алгоритм метода k-средних.

Во-первых, нуобходимо выбрать начальные центры кластеров и поставить нижние границы \( l(x, c) = 0 \) для каждой точки \( x \) и центра \( c \). Присвоить \( x \) ближайшему начальному кластеру \( c(x) = \mathrm{argmin}_c \left[d(x, c)\right] \), используя лемму 1 для избегания избыточных дистанционных вычислений. Каждый раз, когда вычисляется \( d(x, c) \), устанавливать \( l(x, c) = d(x, c) \). Присвоить верхним границам \( u(x) = \min_c d(x, c) \).

Далее, повторять пока метод не сойдется:
\begin{enumerate} \itemsep-2pt
  \item Для всех центров \( c \) и \( c' \), вычислить \( d(c, c') \). Для всех центров \( c \), вычислить \( s(c) = \frac{1}{2} \min_{c'\ne c}d(c, c') \).
  \item Определить все точки \( x \), для которых \( u(x) \le s(c(x)) \).
  \item Для всех остальных точек \( x \) и таких центров \( c \), для которых
     \begin{enumerate} \itemsep-1.5pt
       \item[(i)] \( c \ne c(x) \), и
       \item[(ii)] \( u(x) > l(x, c) \), и
       \item[(iii)] \( u(x) > \frac{1}{2} d(c(x), c) \):
     \end{enumerate}
     \begin{enumerate} \itemsep-1.5pt
       \item[а] если \( r(x) \), то вычислить \( d(x, c(x)) \) и установить \( r(x) = \false \). Иначе, установить \( d(x, c(x)) = u(x) \).
       \item[б] Если \( d(x, c(x)) > l(x, c) \) или \( d(x, c(x)) > \frac{1}{2} d(c(x), c) \), то
         \begin{itemize} \itemsep-1.5pt
           \item[ ] вычислить \( d(x, c) \);
           \item[ ] если \( d(x, c) < d(x, c(x)) \), то присвоить \( c(x) = c \).
         \end{itemize}
     \end{enumerate}
  \item Для каждого центра \( c \), рассчитать \( m(c) \) как среднее всех точек, принадлежащих кластеру \( c \).

  \item Для каждой точки \( x \) и центра \( c \), установить
    \[
      l(x, c) = \max\left\{l(x, c) - d(c, m(c)), 0\right\}.
    \]

  \item Для каждой точки \( x \), установить \( u(x) = u(x) + d(m(c(x)), c(x)) \) и \( r(x) = \true \).

  \item Заменить каждый центр \( c \) на \( m(c) \).
\end{enumerate}

На шаге (3), каждый раз когда вычисляется расстояние \( d(x, c) \) для любых \( x \) и \( c \), его нижняя граница обновляется присвоением \( l(x, c) = d(x, c) \). Точно так же, \( u(x) \) обновляется всякий раз, когда изменяется \( c(x) \) или вычисляется \( d(x, c(x)) \). На шаге (3а), если \( r(x) \) имеет значение \( \true \), то \( u(x) \) признается устаревшим, то есть существует вероятность того, что \( u(x) \ne d(x, c(x)) \). В противном случае, вычисление \( d(x, c(x)) \) не является необходимым. Стадия (3б) повторяет проверки (ii) и (iii) для того, чтобы по возможности избегать вычислений \( d(x, c) \).

Основная причина, по которой алгоритм, приведенный выше, эффективен в уменьшении числа дистанционных измерений, заключается в том, что в начале каждой итерации верхние \( u(x) \) и нижние \( l(x, c) \) границы жестки для большинства точек \( x \) и центров \( c \). Если эти границы являются жесткими в начале одной итерации, обновленные границы, как правило, будут жесткими в начале следующей итерации, поскольку местоположение большинства центров меняется незначительно, а значит и границы изменятся незначительно.

Начальный этап алгоритма сразу прикрепляет каждую точку к его ближайшему кластеру. Это требует относительно много дистанционных вычислений, но это приводит к определению верхних границ \( u(x) \) для всех \( x \) и определению \( l(x, c) \) для большинства пар \( (x, c) \). Альтернативным способом инициализации является прикрепление точек к произвольно выбранному кластеру. Начальные значения \( u(x) \) и \( l(c, x) \) основаны только на расстояниях до выбранного центра. При таком подходе, изначальное число дистанционных измерений сводится к \( n \), но \( u(x) \) и \( l(c, x) \) не так жестко определены, так что в дальнейшем понадобится большее число дистанционных измерений. (После каждой итерации всякая точка всегда верно прикреплена к ближайшему центру, независимо от того насколько неточны значения нижних и верхних границ в начале итерации.) Неофициальные эксперименты показывают, что оба способа инициализации ведут в среднем к одному и тому же значению количества дистанционных измерений.

Логически, шаг (2) является излишним, поскольку его эффект достигает условием (iii). С вычислительной точки зрения, шаг (2) является выгодным, поскольку если он устраняет точку \( x \) от дальнейшего рассмотрения, то сравнение \( u(x) \) и \( l(x, c) \) для каждого \( c \) по отдельности является ненужным. Условие (iii) внутри шага (3) полезно не смотря на шаг (2), поскольку \( u(x) \) и \( c(x) \) могут измениться в ходе выполнения шага (3).

Мы реализовали алгоритм, представленный выше, в среде Matlab. Когда шаг (3) реализован с использованием вложенных циклов, внешний цикл может быть или по \( x \), или по \( c \). Для повышения эффективности в Matlab и похожих языках, внешний цикл должен быть по \( c \), поскольку обычно \( k \ll n \), а внутренний цикл должен быть заменен векторизированным кодом, который работает над всеми соответствующими \( x \) совместно.

Шаг (4) вычисляет новое положение центра каждого кластера \( c \). Расчет \( m(c) \) как среднего всех точек, принадлежащих \( c \) используется тогда, когда в качестве метрики используется евклидово расстояние. В ином случае, \( m(c) \) может рассчитываться по-другому. Например, в методе k-медиан новые центры кластеров являются характерными представителями точек, принадлежащих кластеру.

\chapter{Экспериментальные данные}

В этом разделе представлены результаты работы нового алгоритма на шести больших наборах данных, пять из которых являются высокоразмерными. Наборы данных описаны в таблице 1, а результаты работы алгоритма приведены в таблице 2.

Построение нашего эксперимента схоже с построением эксперимента в статье (Мур, 2000), которая является лучшей работой последнего времени по ускорению работы метода k-средних на высокоразмерных данных. Тем не менее, в (Мур, 2000) использован только один набор данных, необработанные данные которого находятся в открытом доступе, и о котором есть достаточно информации для построения полной картины данных. Этот набор данных назван <<covtype>>. Таким образом, мы также используем еще пять публично доступных наборов данных. Ни один из этих наборов данных не имеет каких-либо недостающих данных.

Для того, чтобы облегчить воспроизводимость наших результатов, мы использовали фиксированную инициализацию для каждого набора данных \( X \). Первый центр инициализируется как среднее значение точек в наборе \( X \). Последующие центры инициализируются в соответствии с эвристикой <<дальше от первого>>: каждый новый центр рассчитывается как \( \mathrm{argmax}_{x \in X} \min_{c \in C} d(x, c) \), где \( C \)~-- это набор полученных на текущий момент центров (Дасгупта, 2002).


Следуя практике прошлых исследований, мы измеряем производительность алгоритма на наборе данных как количество необходимых дистанционных измерений. Все алгоритмы, ускоряющие работу k-средних несут дополнительные расходы на создание и обновление вспомогательных структур данных. Это означает, что ускорение по тактам вычислений всегда меньше, чем по количеству расчетов. Наш алгоритм уменьшает количество дистанционных измерений настолько сильно, что часто его служебное время работы больше, чем время, затраченное на измерения. Тем не менее, общее время выполнения всегда гораздо меньше, чем время, затрачиваемое стандартным методом k-средних. Накладные расходы на создание структур данных \( l \) и \( u \) будут гораздо меньше при реализации алгоритма на C, чем на Matlab, который использовался для проведения экспериментов, представленных здесь. По этой причине, время работы в тактах здесь не сообщается.

Пожалуй, наиболее поразительное наблюдение, сделанное из таблицы 2, состоит в том, что эффективность работы нового алгоритма увеличивается с увеличением \( k \). Число дистанционных измерений растет только с \( k \) и \( e \) (количеств проходов по данным, назваемых <<итерациями>> в таблице 2). Убирается настолько много излишних вычислений, что общее количество измерений становится ближе к \( n \), чем к \( nke \) для стандартного метода k-средних.

Относительное наблюдение состоит в том, что для \( k \ge 20 \) мы получаем гозадо лучшее ускорение, чем с якорным методом, описанном в (Мур, 2000). Ускорения, полученные Муром для набора данных <<covtype>>: 24.8, 11.3, и 19.0 для кластеризации соответственно 3, 20 и 100 кластеров. Ускорения, полученные нами: 8.60, 107 и 310. Мы предполагаем, что увеличение ускорения при \( k \ge 20 \) возникает отчасти от использования фактических центров кластеров как адаптивных <<якорей>>, вместо того, чтобы использовать набор закрепленных якорей при предварительной обработке. Ухудшение ускорения при \( k = 3 \) остается необъясненным.

Еще одним ярким наблюдением является эффективность нового метода даже для данных с высокой размерностью. Мур писал: <<если в данных нет базовой структуры (например, они равномерно распределены), то при рассмотрении данных высоких размерностей ускорение будет либо малым, либо отсутствовать вовсе. Этот мрачный взгляд, поддержанный в недавней теоретической работе (Индик и др., 1999), означает, что мы можем ускорить обработку данных, имеющих интересную внутреннюю структуру>>. В то время, как это утверждение почти наверняка верно для данных, размерность которых асимптотически приближается к бесконечности, наши результаты по набору данных <<random>> показывают, что, по крайней мере при 1000-й размерности данных, ускорение всё еще может быть получено. Как ожидалось, что чем больше кластеров в данных, тем больше получаемое ускорение. Случайная проекция делает кластеры более гауссиановыми (Дасгупта, 2000), так что ускорение лучше для набора данных <<mnist50>>, чем для <<mnist784>>.

\begin{table}
  \footnotesize\centering
  \begin{tabular}{|l|r|r|l|} \hline
    название & количество & размерность & описание \\ \hline
    birch & 100000 & 2 & решетка 10 на 10 гауссиановых кластеров, DS1 в (Чжан и др., 1996) \\ \hline
    covtype & 150000 & 54 & измерения удаленного почвенного покрова (Мур, 2000) \\ \hline
    kddcup & 95413 & 56 & ненормализованные данные соревнования KDD Cup 1998 \\ \hline
    mnist50 & 60000 & 50 & случайная проекция данных цифрового обучения НИСТ \\ \hline
    mnist784 & 60000 & 784 & оригинальные рукописные данные цифрового обучения НИСТ \\ \hline
    random & 10000 & 1000 & равномерные случайные данные \\ \hline
  \end{tabular}
  \caption*{\footnotesize \textit{Таблица 1.} Наборы данных, используемые в экспериментах.}
  
  \vspace{2em}
  
  \begin{tabular}{|llrrr|} \hline
             &             & \( k = 3 \) & \( k = 20 \) & \( k = 100 \) \\ \hline
    birch    & итераций    &          17 &           38 &            56 \\
             & стандартный &   5.100e+06 &    7.600e+07 &     5.600e+08 \\
             & быстрый     &   4.495e+05 &    1.085e+06 &     1.597e+06 \\
             & ускорение   &        11.3 &         70.0 &           351 \\ \hline
    covtype  & итераций    &          18 &          256 &           152 \\
             & стандартный &   8.100e+06 &    7.680e+08 &     2.280e+09 \\
             & быстрый     &   9.416e+05 &    7.147e+06 &     7.353e+06 \\
             & ускорение   &        8.60 &          107 &           310 \\ \hline
    kddcup   & итераций    &          34 &          100 &           325 \\
             & стандартный &   9.732e+06 &    1.908e+08 &     3.101e+09 \\
             & быстрый     &   6.179e+05 &    3.812e+06 &     1.005e+07 \\
             & ускорение   &        15.4 &         50.1 &           309 \\ \hline
    mnist50  & итераций    &          38 &          178 &           217 \\
             & стандартный &   6.840e+06 &    2.136e+08 &     1.302e+09 \\
             & быстрый     &   1.573e+06 &    9.353e+06 &     3.159e+07 \\
             & ускорение   &        4.35 &         22.8 &          41.2 \\ \hline
    mnist784 & итераций    &          63 &           60 &           165 \\
             & стандартный &   1.134e+07 &    7.200e+07 &     9.900e+08 \\
             & быстрый     &   1.625e+06 &    7.396e+06 &     3.055e+07 \\
             & ускорение   &        6.98 &         9.73 &          32.4 \\ \hline
    random   & итераций    &          52 &           33 &            18 \\
             & стандартный &   1.560e+06 &    6.600e+06 &     1.800e+07 \\
             & быстрый     &   1.040e+06 &    3.020e+06 &     5.348e+06 \\
             & ускорение   &        1.50 &         2.19 &          3.37 \\ \hline
  \end{tabular}
  \caption*{\footnotesize \textit{Таблица 2.} Строки ``стандартный'' and ``быстрый'' отображают количество дистанционных измерений совершенных соответственно стандартным методом k-средних и новым алгоритмом. Строки ``ускорение'' показывают во сколько раз быстрее работает новый алгоритм, при условии что единицей измерения является количество дистанционных измерений.}
\end{table}
