\chapter{Методы кластеризации геораспределенных данных}
Геораспределенные данные~--- это данные о событиях, которые в качестве основных атрибутов и атрибутов временной метки имеют две географические координаты~\cite{RFFI}.

При автоматизированном сборе информации о предпочтениях жителей по перемещению возникает потребность работы с большими объемами геораспределенных данных. Поскольку в маршрутной сети количество остановочных пунктов ограничено, то возникает проблема создания выборки данных, оптимальным образом представляющая исходный объем геораспределенных данных. Для этого предлагается использовать методы кластеризации данных.

Стоит заметить, что в работе рассматриваются предпочтения жителей города по перемещению, которые представляют собой пару геораспределенных объектов <<отправление--назначение>>. Однако, при кластеризации дифференциация геораспределенных объектов не происходит~--- все объекты рассматриваются в единой выборке. Это сделано потому, что остановки общественного транспорта, то есть результат кластеризации, не имеют дифференциаций на остановки <<отправления>> и остановки <<назначения>>.

Сформулируем постановку задачи кластеризации геораспределенных данных.

Пусть \( X \)~--- входная выборка точек, полученная из картографических сервисов, \( k \)~--- ожидаемое число узлов (остановочных пунктов) во входной выборке. Требуется построить \( C \)~--- сеть из \( k \) остановочных пунктов, оптимальным образом охватывающие все точки из входной выборки \( X \).

Задача является задачей машинного обучения без учителя, но имеет некоторые специфические особенности. Во-первых, стандартные применяемые метрики для решения данной задачи не подходят: необходимо учитывать особенности местности, такие как реки, овраги и балки, автомобильные и железные дороги, здания и промышленные зоны. Существуют алгоритмы, способные на учет препятствий \cite{estivill, obstacles, cod}, но они не опираются на карту местности, а хранят информацию о препятствиях в некотором абстрактном виде. Во-вторых, поскольку на выходе необходимо получить сеть остановочных пунктов, то центры рассчитываемых кластеров должны находиться на участках дорожной сети, а не в произвольной географической точке.

В качестве базового алгоритма был выбран алгоритм k-средних ввиду его удобной расширяемости на поставленную задачу и вида входных данных, которые можно считать равномерно распределенными по рассматриваемой области, то есть в них нет четко выраженных кластерных структур.

Предлагается два способа расчета расстояний между геораспределенными объектами. Первый способ заключается в использовании в качестве расстояний решение обратной геодезической задачи для рассматриваемых точек \cite{geodesic}. Второй способ заключается в использовании в качестве расстояний длину маршрута между двумя рассматриваемыми точками.

Для начала рассмотрим алгоритмы кластеризации, с помощью которых можно решать данную задачу, если внедрить в них описанные выше методы расчета расстояний и подобрать конкретные параметры управления под конкретную входную выборку.

\section{Общее описание методов}
Можно выделить несколько больших групп алгоритмов по способу кластеризации~\cite{cod}: разделяющие методы, иерархические методы, плотностные методы и сеточные методы.

\subsection{Разделяющие методы}
Разделяющие алгоритмы распределяют объекты заданной \( d \)-мерной выборки \( X \) на заданное число \( k \) кластеров таким образом, что полное отклонение каждого объекта выборки от центра его кластера минимально. Расчет отклонения объекта может производиться по-разному в различных алгоритмах, он более известен как \emph{функция схожести} объектов.

К разделяющим алгоритмам относят алгоритм максимизации ожидания (англ. \emph{Expectation Maximization}), или EM-алгоритм, и различные его производные: методы k-means, k-medoids и другие. Алгоритм k-means использует расчет центров масс объектов в кластере, тогда как алгоритм k-medoids в качестве центра кластера использует самый центральный элемент кластера. EM-алгоритм является вероятностным (или нечетким) методом: он не присваивает объекты определенным кластерам, а говорит, что объект принадлежит всем кластерам, но с некоторой вероятностью~\cite{cod, neiskiy, voron, dbscan-pos}.

Разделяющие алгоритмы можно представить одним общим алгоритмом, представленном на схеме~\ref{alg:EM}.
\begin{algorithm}
    \KwData{Число кластеров \( k \), выборка \( X \)}
    \KwResult{Набор кластеров \( C \) из \( k \) элементов, минимизирующий некоторую функцию-критерий}
    1. Каким-либо образом выбрать начальное расположение \( k \) элементов\;
    2. \While{\( C \) изменяются}{
        1. Рассчитать принадлежность объектов выборки текущему набору кластеров\;
        2. Обновить центры кластеров согласно рассчитанным принадлежностям объектов\;
    }
    \caption{Общий алгоритм разделяющих методов}
    \label{alg:EM}
\end{algorithm}

\subsection{Иерархические методы}
Иерархические методы производят иерархическое разбиение входной выборки, формируя дендрограмму~--- дерево, которое рекурсивно разделяет выборку на всё меньшие группы объектов. Дендрограмма может формироваться двумя методами: <<снизу--вверх>> и <<сверху--вниз>>.

Построение <<снизу--вверх>>, называемое агломерационным, начинается с разбиения всех объектов на отдельные группы. Далее происходит объединение близлежащих по каким-либо метрикам объектов в б\'{о}льшие группы. Алгоритм останавливается, когда все объекты выборки образуют одну группу.

Построение <<сверху--вниз>>, называемое разделяющим, происходит с точностью наоборот: сначала все объекты образуют одну большую группу, которая по каким-либо метрикам отделяет далекие группы друг от друга. Алгоритм останавливается, когда все объекты выборки образуют отдельные кластеры.

При работе с иерархическими методами встает вопрос об определении <<расстояния>> между группами для объединения или разбиения~\cite{voron}. Существует формула Ланса--Уильямса~\eqref{eq:LansWill}, подбором параметров которой можно добиваться различных результатов:
\begin{equation}
    R(W, S) = \alpha_UR(U, S) + \alpha_VR(V, S) + \beta R(U, V) + \gamma\abs{R(U, S) - R(V, S)},
    \label{eq:LansWill}
\end{equation}
где \( U \), \( V \)~--- объединяемые кластеры, \( W = U\cup V \)~--- объединенный кластер, \( S \)~--- другие кластеры, \( R(a, b) \)~--- расстояние между элементами, \( \alpha_U \), \( \alpha_V \), \( \beta \), \( \gamma \)~--- числовые параметры. Есть несколько частных случаев формулы Ланса--Уильямса:
\begin{enumerate}
    \item расстояние ближнего соседа (рис.~\ref{pic:LansWill}а):
         \( \beta = 0 \), \( \gamma = -1/2 \),\\
         \( \alpha_U = \alpha_V = 1/2 \);
         \( \ds R_\text{Б}(W, S) = \min_{w\in W,s\in S}\rho(w, s) \);
    \item расстояние дальнего соседа (рис.~\ref{pic:LansWill}б):
         \( \beta = 0 \), \( \gamma = 1/2 \),\\
         \( \alpha_U = \alpha_V = 1/2 \);
         \( \ds R_\text{Д}(W, S) = \max_{w\in W,s\in S}\rho(w, s) \);
    \item групповое среднее расстояние (рис.~\ref{pic:LansWill}в):
         \( \beta = \gamma = 0 \),\\
         \( \ds\alpha_U = \frac{\abs{U}}{\abs{W}} \),
         \( \ds\alpha_V = \frac{\abs{V}}{\abs{W}} \);         
         \( \ds R_\text{Г}(W, S) = \frac{1}{\abs{W}\abs{S}}\sum_{w\in W}\sum_{s\in S}\rho(w, s) \);
    \item расстояние между центрами (рис.~\ref{pic:LansWill}г):
         \( \beta = -\alpha_U\alpha_V \), \( \gamma = 0 \),\\
         \( \ds\alpha_U = \frac{\abs{U}}{\abs{W}} \),
         \( \ds\alpha_V = \frac{\abs{V}}{\abs{W}} \);
         \( \ds R_\text{Ц}(W, S) = \rho^2\left[\sum_{w\in W}\frac{w}{\abs{W}},\sum_{s\in S}\frac{s}{\abs{S}}\right] \).
\end{enumerate}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=.22\textwidth]{LW_1}\
    \includegraphics[width=.22\textwidth]{LW_2}\
    \includegraphics[width=.22\textwidth]{LW_3}\
    \includegraphics[width=.22\textwidth]{LW_4}\\[1ex]
    \parbox{.22\textwidth}{\small\centering а)}\
    \parbox{.22\textwidth}{\small\centering б)}\
    \parbox{.22\textwidth}{\small\centering в)}\
    \parbox{.22\textwidth}{\small\centering г)}\\
    \parbox{.9\textwidth}{\caption{Иллюстрации частных случаев расчета расстояния по формуле~\eqref{eq:LansWill}}\label{pic:LansWill}}
\end{figure}

Не смотря на простую идею, у первых алгоритмов были существенные проблемы с критериями принятия решений о слиянии или разделении групп, одно неверное решение приводило к дальнейшим неправильным результатам и фактически губило правильные результаты, полученные ранее.

К иерархическим алгоритмам относят следующие алгоритмы: алгоритмы AGNES и DIANA, представляющие собой самые первые версии иерархических алгоритмов, в них соответственно реализованы агломерационная и разделяющие построения; алгоритмы CURE и CHAMELEON, разработанные для устранения недостатков первых алгоритмов, и современные алгоритмы, такие как BIRCH~\cite{cod, neiskiy, voron}.

\subsection{Плотностные методы}
Большинство разделяющих методов построено на расчете расстояний между объектами. Такой подход позволяет искать кластеры только сферической формы, а результаты поиска кластеров другой формы оказываются неудовлетворительными. Поскольку типы кластерных структур в данных могут быть различными (рис.~\ref{pic:clusters}), то необходим иной подход для выделения кластеров в данных.
\begin{figure}[h!]
    \centering
    \includegraphics[width=.9\textwidth]{clusters}\\[1ex]
    \parbox{\textwidth}{\caption{Типы кластерных структур (рисунок взят из курса лекций Воронцова К.~В.~\cite{voron})}\label{pic:clusters}}
\end{figure}

Этим подходом стал анализ плотности объектов в пространстве. В плотностых методах кластеры рассматриваются как сгустки плотности, между которыми находится разреженное пространство. Такой подход позволяет не только находить кластеры различной формы, но и отсеивать шум в данных.

К плотностным алгоритмам относят алгоритмы DBSCAN, OPTICS, DENCLUE и другие. Такие алгоритмы требуют тщательного подбора входных параметров, с помощью которых определяется плотность в пространстве вокруг объектов выборки: неправильно заданные параметры могут привести либо к получению одного кластера, содержащего всю выборку, либо к множеству мелких кластеров. Подробнее плотностный алгоритм DBSCAN будет рассмотрен в разделе~\ref{sec:dbscan}.

Плотностные алгоритмы, подобные DBSCAN и OPTICS, существенно теряют в производительности при высокой размерности данных. Алгоритм DENCLUE разрабатывался для уменьшения потери производительности~\cite{cod, neiskiy}.

\subsection{Сеточные методы}
Чтобы увеличить производительность при обработке высокоразмерных данных в сеточных алгоритмах используется сеточная структура данных. Она представляет собой пространство, поделенное на конечное число ячеек, над которыми производится операция кластеризации. Основное преимущество такого подхода~--- быстрое время обработки, которое обычно не зависит от числа объектов выборки, а зависит от числа ячеек в каждой плоскости сеточной структуры. К сеточным алгоритмам относят алгоритмы STING, WaveCluster, CLIQUE и другие.

Алгоритм STING делит пространство на квадратные ячейки. Обычно используется несколько уровней таких ячеек, имеющих иерархическую структуру: каждая ячейка верхнего уровня разделяется на несколько ячеек более нижнего уровня. В каждой ячейке хранятся общие данные, рассчитанные по области ячейки. Данные о ячейке состоят из количества элементов в ячейке, минимального, максимального и среднего значений, среднеквадратического отклонения, а также тип распределения данных в ячейке. Для проведения кластеризации пользователь должен задать \emph{уровень плотности}, используя который алгоритм проведет агломерационное разбиение данных, выбирая подходящие по плотности ячейки.

Алгоритм CLIQUE объединяет плотностный и сеточный подходы. В отличие от других алгоритмов, он способен находить кластеры в подпространствах данных, что удобно при работе с многомерными данными, которые обычно являются разреженными и не формируют кластеров в полном пространстве. С точки зрения обнаружения кластеров используется плотностный подход~--- пользователь определяет количество объектов, расположенных поблизости, для восприятия их как кластер. С точки зрения проведения кластеризации используется сеточный подход~--- пространство разбито на ячейки, и при поиске в \( d \)-мерном пространстве алгоритм обобщает информацию, собранную в \( (d - 1) \)-мерном пространстве~\cite{cod}.

\section{Проанализированные методы}
\subsection{Алгоритм Mean Shift}
Алгоритм Mean Shift (средний сдвиг)~--- это процедура определения положения максимумов функции плотности. Часто используется в работах по кластеризации, обработке изображений и визуальном трекинге \cite{ms, meanshift}. Пример работы можно посмотреть на рисунке~\ref{pic:km-ms}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.97\textwidth]{km_ms}\\[1ex]
    \parbox{.9\textwidth}{\caption{Результаты обработки одинаковых наборов данных алгоритмами k-means (верхний ряд) и mean shift (нижний ряд). Рисунок получен с использованием библиотеки scikit-learn~\cite{sklearn}} \label{pic:km-ms}}
\end{figure}

Пусть \( S\subset X \)~--- конечная выборка \( d \)-размерного пространства \( X \) из \( n \) элементов, \( K \)~--- функция ядра, \( w \)~--- весовая функция. <<Среднее выборки>> с ядром \( K \) в точке \( x\in X \) определяется функцией:
\begin{equation}
    m(x) = \frac{\ds\sum_{s\in S} K(s - x)w(s)s}{\ds\sum_{s\in S} K(s - x)w(s)}.
    \label{eq:wiki-shift}
\end{equation}
Пусть \( T\subset X \)~--- конечное множество центров кластеров. Изменение \( T \) итерационным присвоением \( T\gets m(T) = \Big\{ m(t), t\in T\Big\} \) называется алгоритмом среднего сдвига, или Mean Shift. Алгоритм останавливается, когда достигает фиксированного множества \( m(T) = T \) \cite{meanshift}.

Алгоритм основан на парзеновской оценке плотности:
\begin{equation}
    p_h(x) = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x - s}{h}\right),
    \label{eq:voron2.10}
\end{equation}
где \( h \)~--- неотрицательный параметр, называемый \emph{шириной окна}. Эмпирическая оценка плотности определяется как доля точек выборки \( S = (s)_{i=1}^n \), лежащих внутри отрезка \( [x - h, x + h] \).

Параметр \( h \) в формуле~\eqref{eq:voron2.10} называют шириной окна (пропускной способностью, англ. \emph{bandwidth}) алгоритма.

Ширина окна существенно влияет на качество определения сгустков плотности.

При \( h\to 0 \) сгустами плотности считаются все объекты выборки, функция~\eqref{eq:voron2.10} начинает претерпевать резкие скачки. При \( h\to\infty \) функция плотности чрезмерно сглаживается и вырождается в константу~\cite{voron}.

При сильно неравномерном распределении объектов выборки в пространстве \( X \) одно и то же значение ширины окна \( h \) может привести к недостаточному сглаживанию плотности в одних областях пространства \( X \) и наоборот, к сильному сглаживанию плотности в других областях. Для решения этой проблемы используют переменную ширину окна, определяемую в каждой точке \( x \) пространства \( X \). Тогда при расчетах используется матрица пропускной способности \( \matx{H} \)~--- симметричная положительная матрица размером \( d\times d\):
\begin{equation}
    f(x) = \frac{1}{n}\sum_{i=1}^n K_\matx{H}(x - s),
    \label{eq:msrobust1}
\end{equation}
где \( f(x) \)~--- функция оценки плотности, ядро \( K_\matx{H} \) представляется в виде:
\[
    K_\matx{H}(z) = \abs{\matx{H}}^{-1/2} K(\matx{H}^{-1/2}z).
\]

Зачастую при рассчетах матрицу \( \matx{H} \) считают диагональной:\linebreak \( \matx{H} = \matx{diag}\left(h_1^2, \ldots, h_d^2\right) \). При постоянной ширине окна матрицу можно представить в виде \( \matx{H} = h^2\matx{E} \), где \( \matx{E} = \matx{diag}(1, 1, \ldots) \)~--- единичная матрица.

Функция ядра \( K(z) \) является четной, нормированной и непрерывной, удовлетворяет следующим условиям:

\[
    \begin{array}{rl}
    \ds \int_X K(z)\,dz = 1; \qquad &
    \lim_{\norm{z}\to\infty} \norm{z}^d K(z) = 0; \\[1em]
    \ds \int_X zK(z)\,dz = 0; \qquad &
    \int_X zz^\T K(z)\,dz = c_K\matx{E},
    \end{array}
\]
где \( c_K \)~--- константа.

В алгоритме используется \emph{профильный} способ представления ядра, которым можно описать радиально-симметричные ядра, удовлетворяющие условию
\begin{equation}
    K(z) = c_{k, d} k\left(\norm{z}^2\right).
    \label{eq:msrobust5}
\end{equation}

Функция \( k(z) \) называется профилем ядра, \( c_{k,d} > 0 \)~--- константа нормализации.

Подставляя \eqref{eq:msrobust5} в \eqref{eq:msrobust1} и считая ширину окна постоянной и равной \( h \), получим функцию оценивания плотности в следующем виде:
\begin{equation}
    f_{h, K}(x) = \frac{c_{k, d}}{nh^d}\sum_{i=1}^n k\left(\norm{\frac{x - s}{h}}^2\right).
    \label{eq:msrobust11}
\end{equation}

Хотя функция ядра практически не влияет на качество кластеризации, она определяет степень гладкости функции~\eqref{eq:msrobust11}, и ее вид может повлиять на эффективность вычислений.

Зачастую для алгоритма Mean Shift используют определенные профили ядерных функций:
\begin{itemize}
    \itemsep-.5ex
    \item Плоское ядро:
        \( \ds k(z) = \left\{
            \begin{array}{ll}
                1 & \text{ если } z\le\lambda, \\
                0 & \text{ если } z > \lambda;
            \end{array} \right.
        \)
    \item нормальное (гауссовское) ядро:
        \( \ds k(z) = \exp\left(-\frac{\norm{z}^2}{2\sigma^2}\right), \)
        в котором параметр среднеквадратичного отклонения \( \sigma \) играет роль ширины окна \( h \);
    \item ядро Епачечникова:
        \( \ds k(z) = \left\{
            \begin{array}{ll}
                1 - z & \text{ если } 0\le z\le 1, \\
                0 & \text{ если } z > 1.
            \end{array} \right.
        \)
\end{itemize}

Формула~\eqref{eq:wiki-shift} представляет собой упрощенную запись отношения\linebreak \( m(x) = c\nabla f(x) / f(x) \). Выражение \( \nabla f \) является градиентом функции плотности:
\[
    \nabla f_{h, K}(x) = \frac{2c_{k, d}}{nh^{d+2}}\sum_{i=1}^n (x - s)k'\left(\norm{\frac{x - s}{h}}^2\right).
\]

Введя замену \( g(z) = -k'(z) \), получим следующее:
\[
    \nabla f_{h, K}(x) = f_{h, G}(x)\frac{2c_{k, d}}{h^2c_{g, d}}m_{k, G}(x),
\]
где \( G(z) = c_{g, d} g\left(\norm{z}^2\right) \), \( m_{k, G} \)~--- средний сдвиг. Отсюда получаем:
\begin{equation}
    m_{k, G}(x) = \frac{h^2c_{g, d}}{2c_{k, d}}\frac{\nabla f_{h, K}(x)}{f_{h, G}}.
    \label{eq:themeanshift}
\end{equation}

Таким образом, алгоритм на каждой итерации производит расчеты функций оценки плотности и градиентов функций оценки плотности в точках, считающихся центрами кластеров; после чего рассчитывает значение среднего сдвига и сдвигает центры кластеров на эту величину

Псевдокод алгоритма~\cite[с. 235-236]{algms} представлен на схеме~\ref{alg:meanshift}.
\begin{algorithm}[ht!]
    \caption{Алгоритм Mean Shift}
    \KwData{\( S \), \( h \).}
    \KwResult{\( C = \{C_j\}\)~--- список центров кластеров.}
    \For{каждого центра \( C_j \)}{
        1. \For{каждой точки данных \( s \)}{
            Рассчитать градиент плотности
            \[
               \nabla f_{h, K}(C_j) = \frac{2c_{k, d}}{nh^{d+2}}\sum_{i=1}^n(s - C_j)g\left(\norm{\frac{C_j - s}{h}}^2\right)\text{\;}
            \]
        }
        2. Сдвинуть позицию согласно~\eqref{eq:themeanshift}: \( C_j = C_j + m_{k, G}(C_j) \)\;
        3. \lIf{центры кластеров \( C \) изменились}{перейти на шаг 1}\lElse{закончить выполнение}
    }
    \label{alg:meanshift}
\end{algorithm}

\subsection{Алгоритм DBSCAN} \label{sec:dbscan}
Алгоритм DBSCAN (англ. \emph{Density-Based Spatial Clustering of Application with Noise}, плотностный алгоритм кластеризации пространственных данных с присутствием шума) был предложен как решение проблемы разбиения данных на кластеры произвольной формы.

Поскольку большинство алгоритмов стараются минимизировать расстояние от элементов до центра кластера, то они создают кластеры, по форме приближенные к сферическим. Авторы DBSCAN экспериментально показали, что разработанный алгоритм способен распознать кластеры различной формы. Пример работы алгоритма можно посмотреть на рисунке~\ref{pic:km-dbscan}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.97\textwidth]{km_dbscan}\\[1ex]
    \parbox{.9\textwidth}{\caption{Результаты обработки одинаковых наборов данных алгоритмами k-means (верхний ряд) и DBSCAN (нижний ряд). Рисунок получен с использованием библиотеки scikit-learn~\cite{sklearn}} \label{pic:km-dbscan}}
\end{figure}

Алгоритм управляется двумя параметрами: \( \varepsilon \)~--- радиус области, на которой будет проводиться поиск соседних элементов, и \( \mu \)~--- минимальное количество соседних элементов в \( \varepsilon \)-окрестности, при котором элемент считается \emph{ядровым} элементом.

Основной концепт, положенный в основу работы алгоритма, заключается в том, что внутри каждого кластера плотность объектов обычно заметно выше, чем плотность объектов снаружи кластера, и плотность объектов в областях с шумом гораздо ниже плотности любого из кластеров~\cite{dbscan-pos}.

Реализация этого концепта была выражена следующими правилами~\cite{cod}:
\begin{itemize}
    \item объект может принадлежать кластеру только в том случае, если он лежит в \( \varepsilon \)-окрестности одного из ядровых элементов кластера;
    \item ядровый элемент \( o \), лежащий в \( \varepsilon \)-окрестности другого ядрового элемента \( p \), должен принадлежать тому же кластеру, что и \( p \);
    \item неядровый элемент \( q \), лежащий в \( \varepsilon \)-окрестности нескольких ядерных элементов \( p_1, \ldots, p_i \), должен принадлежать к тому же кластеру, к которому принадлежит хотя бы один из ядровых элементов;
    \item неядровый элемент \( r \), не лежащий в \( \varepsilon \)-окрестности ядровых элементов, считается шумовым элементом~--- элементом, не принадлежащему ни одному кластеру.
\end{itemize}

\begin{algorithm}[t!]
    \DontPrintSemicolon
    \KwData{\( X \), \( \varepsilon \), \( \mu \).}
    \KwResult{\( C = \{C_j\} \)~--- список кластеров.}
    1. Установить всем элементам \( X \) флаг = <<не посещен>>; \( j = 0 \); множество шума: \( N = \varnothing \);\;
    2. \ForEach{\( x\in X \) такого, что флаг == <<не посещен>>}{
        1. Флаг(\( x \)) = <<посещен>>\;
        2. \( n_x = \mathrm{Eps}(x) = \{q\in X | \rho(x, q)\le \varepsilon\}\);\;
        3. \lIf{\( \abs{n_x} < \mu \)}{N = N + \{x\}}\Else{
            номер следующего кластера \( j = j + 1\);\;
            \( \mathrm{ExpandCluster}(x, n_x, C_j, \varepsilon, \mu) \);\;
        }
    }
    \BlankLine
    \( \mathrm{ExpandCluster} \)\;
    \KwData{\( x \), \( n_x \), \( C_j \), \( \varepsilon \), \( \mu \).}
    \KwResult{кластер \( C_j \).}
    1. \( C_j = C_j + \{x\} \);\;
    2. \ForEach{\( q\in n_x \)}{
        1. \If{Флаг(q) == <<не посещен>>}{
            1. Флаг(q) == <<посещен>>;\;
            2. \( n_q = \mathrm{Eps}(q) \);\;
            3. \lIf{\( \abs{n_q}\ge \mu \)}{\( n_x = n_x + \{n_q\} \)}
        }
        2. \lIf{элемент \( q \) не принадлежит ни одному из кластеров}{\( C_j = C_j + \{q\} \)}
    }
    \caption{Алгоритм DBSCAN}
    \label{alg:dbscan}
\end{algorithm}

Чтобы обнаружить кластеры в данных, алгоритм DBSCAN просматривает \( \varepsilon \)-окрестности всех элементов входной выборки \( X \). Если в \( \varepsilon \)-окрестности элемента \( p \) находится больше, чем \( \mu \) элементов, то создается новый кластер с ядровым элементом \( p \). Все элементы в \( \varepsilon \)-окрестности ядрового элемента \( p \) добавляются в созданный кластер. Новые ядровые элементы в \( \varepsilon \)-окресности элемента \( p \) и элементы из их \( \varepsilon \)-окрестности также добавляются в созданный кластер; таким образом кластер разрастается. Как только в \( \varepsilon \)-окрестностях добавленных ядровых объектов больше нет элементов, то из выборки \( X \) берется следующий нерасмотренный ядровый элемент и создается новый кластер. Стоит отметить, что во время работы алгоритма ядровые элементы одного кластера могут быть обнаружены при разрастании другого, что приведет к слиянию кластеров. Алгоритм прекращает работу, когда нет новых точек, которые могут быть добавлены к какому-либо из кластеров.

Псевдокод алгоритма~\cite[с. 199]{dbscan-pos} представлен на схеме~\ref{alg:dbscan}. Используются следующие обозначения: \( \mathrm{Eps} \)~--- множество соседей элемента, находящихся от точки \( p \) на расстоянии не более \( \varepsilon \), \( N \)~--- множество элементов шума.

Существует улучшенная версия алгоритма DBSCAN, названная OPTICS (англ. \emph{Ordering Points To Identify Clustering Structure}, сортировка точек для определения кластерной структуры)~\cite{cod}. Он был создан для того, чтобы не запускать алгоритм DBSCAN множество раз с различными параметрами для получения оптимальной с точки зрения пользователя кластерной структуры. OPTICS также требует определения входных параметров \( \varepsilon \) и \( \mu \), но, в отличие от DBSCAN, при работе сортирует точки таким образом, чтобы легко визуализировать и рассчитать кластерную структуру для данного \( \varepsilon \) и некоторых других значений \( \varepsilon' < \varepsilon \).

\subsection{Алгоритм BIRCH}
Алгоритм BIRCH (англ. \emph{Balanced Iterative Reducing and Clustering using Hierarchies}, сбалансированное итерационное сокращение и кластеризация с использованием иерархий) создан для работы с большими наборами данных.

Пример работы алгоритма можно посмотреть на рисунке~\ref{pic:km-birch}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.97\textwidth]{km_birch}\\[1ex]
    \parbox{.9\textwidth}{\caption{Результаты обработки одинаковых наборов данных алгоритмами k-means (верхний ряд) и BIRCH (нижний ряд). Рисунок получен с использованием библиотеки scikit-learn~\cite{sklearn}} \label{pic:km-birch}}
\end{figure}

Основная идея работы алгоритма BIRCH~--- сократить входную выборку до некоторого количества субкластеров, а затем провести кластеризацию над этими субкластерами~\cite{cod}. Из-за усечения множества исходных данных количество субкластеров гораздо меньше, чем число элементов входных данных, следовательно, это позволяет производить кластеризацию с меньшим количеством используемой памяти.

В алгоритме каждый небольшой субкластер представляется \emph{кластерным элементом} (англ. \emph{clustering feature}), который является триплетом, обобщающим информацию о группе объектов выборки, которую заменяет субкластер. Если в заданном \( d \)-размерном пространстве \( X \) субкластер заменяет \( N \) объектов выборки, то кластерный элемент определяется как
\[
    CF = \left(N, \vec{LS}, SS\right),
\]
где \( N \)~--- число заменяемых объектов выборки, \( \ds\vec{LS} = \sum_{i=1}^N \vec{X}_i \)~--- линейная сумма заменяемых объектов, \( \ds SS = \sum_{i=1}^N \vec{X}_i^2 \)~--- сумма квадратов заменяемых объектов.

Кластерные элементы хранятся в \emph{дереве кластерных элементов}, которое используется для иерархической кластеризации. Внутренние узлы дерева хранят суммы кластерных элементов их потомков и, следовательно, обобщенную информацию о потомках. Дерево кластерных элементов имеет два параметра: коэффициент ветвления \( B \) и пороговое значение \( T \). Коэффициент ветвления определяет максимальное количество потомков у внутренних узлов дерева. Пороговое значение определяет максимальный диаметр субкластеров, хранимых в листовых узлах дерева. Эти два параметра влияют на результирующий размер дерева.

Поскольку алгоритм рассчитан на работу с очень большими данными, то дерево строится сразу по мере считывания входной базы данных. Объекты вставляются в ближайший листовой элемент (субкластер). Если диаметр субкластера после добавления нового объекта становится больше порогового значения, то листовой узел разделяется и становится внутренним узлом. После добавления нового объекта информация о нем передается в корень дерева.

Если результирующее дерево выходит за границы доступной памяти, то изменяется пороговое значение \( T \) и дерево перестраивается, начиная с корня. Таким образом, процесс перестройки дерево происходит без повторного считывания всей исходной выборки.

После постройки результирующего дерева для кластеризации может использоваться любой алгоритм кластеризации, который не превысит границ доступной памяти.

Не смотря на достоинства алгоритма~--- линейную масштабируемость, хорошее качество кластеризации преобразованных данных,~--- узел в дереве кластерных элементов может содержать лишь ограниченное количество элементов из-за ограничения по размеру, что влечет за собой тот факт, что кластерное образование, наблюдаемое пользователем, может быть не отражено в дереве. Более того, если кластеры имеют несферическую форму, то алгоритм BIRCH не может корректно отразить этот факт, поскольку контроль за границами кластеров ведется через понятия радиусов и диаметров~\cite{cod, birch}.

Общее описание алгоритма~\cite[с.~2]{neiskiy} приведено на схеме~\ref{alg:birch}.

\begin{algorithm}
    \DontPrintSemicolon
    Фаза 1. Загрузка данных в память.\;
    Построение начального дерева по данным (сканирование выборки).\;
    Фаза 2 (необязательная). Сжатие данных до приемлимых размеров.\;
    Перестройка и уменьшение дерева кластеных элементов с увеличением порогового значения \( T \).\;
    Фаза 3. Глобальная кластеризация.\;
    Применение выбранного алгоритма кластеризации на листовых элементах дерева.\;
    Фаза 4 (необязательная). Улучшение кластеров.\;
    Перераспределение данных между близкими кластерами, используя центры тяжести кластеров, полученные в фазе 3, как основы.\;
    \caption{Общая схема алгоритма BIRCH}
    \label{alg:birch}
\end{algorithm}

\vspace{-2ex}
\section{Алгоритм k-means} \label{sec:kmeans}
Так как алгоритм BIRCH чаще всего используют именно для работы с очень большими данными, о которых можно иметь репрезентативное представление в виде кластерных элементов~\cite{cod, birch, neiskiy}, то для кластеризации геораспределенных данных этот алгоритм не применялся.

Поскольку в работе рассматриваются данные, приближенные (но не являющиеся таковыми) к равномерно распределенным, то результатом работы алгоритма DBSCAN будет являться небольшое количество кластеров, включающих в себя большое количество точек.

Точно такое же поведение будет наблюдаться у алгоритма Mean Shift, но он более удобно настраивается, поскольку управляется одним параметром~--- шириной окна \( h \). При уменьшении этого параметра алгоритм проводит более детальную кластеризацию, однако результаты его работы слабо отличаются от результатов работы алгоритма k-means (рис.~\ref{pic:oldkm-ms}).
\begin{figure}[t!]
    \centering
    \includegraphics[width=.8\textwidth]{old_km}\\[.5ex]
    \includegraphics[width=.8\textwidth]{old_ms}\\[1ex]
    \parbox{.9\textwidth}{\caption{Результаты работы алгоритмов k-means (верхний) и Mean Shift (нижний) на сгенерированной входной выборке. Широкими кругами обозначены центры кластеров, радиус круга зависит от количества входящих в него точек}\label{pic:oldkm-ms}}
\end{figure}

\subsection{Общее описание}
Алгоритм k-means (k-средних) строит \( k \) кластеров, расположенных таким образом, чтобы минимизировать среднеквадратичное отклонение объектов выборки от центров кластеров. Выбор числа \( k \) может базироваться на результатах теоретических соображений, предшествующих исследований, визуального наблюдения или иных источниках информации~\cite{neiskiy}.

Идеальным кластером алгоритм k-means считает сферу с центром кластера в центре сферы~\cite{dbscan-pos}.

При известном числе кластеров алгоритм k-средних начинает работу с некоторого начального расположения кластеров, к которым присваиваются объекты выборки.

Начальное расположение кластеров очень сильно влияет на работу алгоритма. Из-за неправильного выбора начальных значений алгоритм может попасть в некоторый локальный минимум целевой функции (рис.~\ref{pic:local_fail}).

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.45\textwidth]{local_fail} \hspace{1em}
    \includegraphics[width=.45\textwidth]{local_fail-2} \\[1ex]
    \parbox{.9\textwidth}{\caption{Пример сходимости алгоритма к локальному минимуму. Кружками обозначены объекты выборки, звездами~--- центры кластеров. Рисунок получен с помощью апплета Миркеса~\cite{mirkes}} \label{pic:local_fail}}
\end{figure}

Целевой функцией алгоритма является среднеквадратичное расстояние (при использовании евклидовой метрики) между объектами выборки и центрами их кластеров:
\[
    f(X, C) = \sum_{j=1}^k \sum_{i=1}^n \norm{x_i - \mu_j}^2,
\]
где \( \mu_j \)~--- центр кластера \( C_j \), вычисляющийся по формуле:
\begin{equation}
    \mu_j = \frac{1}{\abs{C_j}} \sum_{i=1}^n x_i.
    \label{eq:km-mu}
\end{equation}

Начальное расположение кластеров может задаваться как вручную, так и автоматически. В изначальном варианте алгоритма начальное расположение кластеров было случайным~\cite{macqueen}. На практике в качестве начальных центров кластеров зачастую либо выбирают первые \( k \) объектов выборки, либо выбирают \( k \) самых отдаленных друг от друга объектов выборки.

Алгоритм k-средних относительно масштабируем и эффективен при обработке не слишком больших баз данных поскольку его вычислительная сложность равна \( O(nki) \), где \( n \)~--- полное число объектов выборки, \( k \)~--- число кластеров, \( i \)~--- число итераций алгоритма; обычно \( k\ll n \) и \( i\ll n \)~\cite{cod}.

Недостатками алгоритма являются:
\begin{itemize}
    \item чувствительность к выбросам~--- точки, далеко находящиеся от всех кластеров, сильно искажают среднее;
    \item необходимость задавать количество кластеров~--- при работе с алгоритмом выбор числа кластеров является самым сложным вопросом. Если нет предположений о кластерной структуре данных, то рекомендуют постепенно наращивать число \( k \), сравнивая полученные результаты~\cite{neiskiy};
    \item необходимость сканировать всю выборку для определения положения каждого кластера~--- при работе с большими базами данных это сильно замедляет работу.
\end{itemize}

Достоинствами метода являются его простота и быстрота использования, а также понятность и прозрачность алгоритма.

\subsection{Объяснение работы}
Работу алгоритма можно разбить на три этапа: начальный этап, этап присвоения и этап сдвига.

\begin{figure}[h!]
    \centering
    \raisebox{-1.5ex}{\includegraphics[width=.2\textwidth]{km_step1}} \hspace{1ex}
    \includegraphics[width=.2\textwidth]{km_step2} \hspace{1ex}
    \includegraphics[width=.2\textwidth]{km_step3} \hspace{1ex}
    \includegraphics[width=.2\textwidth]{km_step4} \\
    \parbox{.2\textwidth}{\centering\small а)} \hspace{1ex}
    \parbox{.2\textwidth}{\centering\small б)} \hspace{1ex}
    \parbox{.2\textwidth}{\centering\small в)} \hspace{1ex}
    \parbox{.2\textwidth}{\centering\small г)} \\[1ex]
    \parbox{.9\textwidth}{\caption{Демонстрация работы алгоритма. Иллюстрация взята из статьи о k-means~\cite{wiki}}\label{pic:km_steps}}
\end{figure}

На начальном этапе происходит выбор числа \( k \), расстановка начальных центров кластеров \( C_0 \) (рис.~\ref{pic:km_steps}а).

На этапе присвоения происходит расчет расстояния от каждого объекта выборки \( X \) до каждого объекта кластера \( C_j \). После чего считается, что объект принадлежит тому кластеру, до которого у него минимальное расстояние (рис.~\ref{pic:km_steps}б).

На этапе сдвига происходит расчет нового положения центров кластеров по формуле~\eqref{eq:km-mu} (рис.~\ref{pic:km_steps}в).

Этапы присвоения и сдвига повторяются, пока кластерные центры не стабилизируются, то есть все объекты, принадлежащие кластеру на прошлой итерации, принадлежат и на этой, либо пока не достигнется заданное максимальное число итераций \( I \). На выходе алгоритм выдает множество центров кластеров \( C \) и метки принадлежности объектов выборки кластерам \( L \) (рис.~\ref{pic:km_steps}г).

Псевдокод алгоритма представлен на схеме~\ref{alg:kmeans}.
\begin{algorithm}
    \KwData{\( X \), \( k \), \( C_0 \), \( I \).}
    \KwResult{\( C \), \( L \).}
    1. \( C = C_0 \), \( L_0 = \varnothing \), \( i = 0\)\;
    2. \While{\( L != L_0\) и \( i < I \)}{
        1. \( L_0 = L \), \( A = \varnothing \), \( \mu = \varnothing \)\;
        2. \ForEach{\( x \in X \)}{
            1. \( r = \varnothing \)\;
            2. \ForEach{\( c \in C \)}{
                1. Рассчитать \( r_{xc} = \rho(x, c) \)\;
                2. \( r = r + \{ r_{xc} \} \)\;
            }
            3. \( L[x] = C[\max(r)] \)\;
            4. \( A[C[\max(r)]] = A[C[\max(r)]] + \{x\}\)\;
        }
        3. \ForEach{\( c \in C \)}{
            1. \( \mu[c] = \sum(A[c])\,/ \abs{A[c]} \)\;
            2. \( c = \mu[c] \)\;
        }
        4. \( i = i + 1 \)\;
    }
    \caption{Алгоритм k-means}
    \label{alg:kmeans}
\end{algorithm}

\section{Расчет расстояний между точками} \label{sec:distance}

Особая метрика при работе алгоритма необходима по той причине, что два близких по стандартным метрикам геораспределенных объекта в реальности могут преграждаться препятствиями. Примеры таких объектов приведены на рисунке~\ref{pic:2points}. На рисунке~\ref{pic:2points-1} приведено реальное расстояние между объектами, которое должно учитываться при транспортных расчетах.

Для расчета расстояния между точками в работе используются две метрики, названные <<Surface>> и <<Route>>.

Метрика \emph{Surface} рассчитывает расстояние путем решения обратной геодезической задачи~\cite[с. 48-50]{geodesic}. Метрика \emph{Route} рассчитывает расстояние между объектами по графу городских дорог.

\begin{figure}[t!]
    \centering
    \includegraphics[width=.47\textwidth]{2points_1} \hspace{1ex}
    \includegraphics[width=.47\textwidth]{2points_3} \\[.5ex]
    \parbox{.9\textwidth}{\caption{Близкие по евклидовой метрике пары объектов}\label{pic:2points}}
    \includegraphics[width=.47\textwidth]{2points_2} \hspace{1ex}
    \includegraphics[width=.47\textwidth]{2points_4} \\[.5ex]
    \parbox{.9\textwidth}{\caption{Расстояние между объектами, которое должно учитываться при расчете}\label{pic:2points-1}}
\end{figure}

Для решения обратной геодезической задачи в метрике \emph{Surface} используется библиотека \emph{GeographicLib}~\cite{geographiclib}. Метрика \emph{Route} использует сервис построения маршрутов \emph{Open Source Routing Machine} (OSRM)~\cite{OSRM} для расчета расстояния между точками по городским дорогам.

Сервис OSRM предоставляет открытый код своего движка маршрутизации. Для работе с собранным движком на пользовательском компьютере используются HTTP-запросы определенной структуры, так называемое HTTP-API сервиса (поддерживается две версии API: v4 и v5). Результат обработки запроса выводится в json-формате, содержащем всю информацию по запросу.

В метрике используются два типа запросов: \emph{route} (\emph{viaroute} в версии HTTP-API v4) для расчета расстояний и \emph{nearest} (\emph{locate} в версии HTTP-API v4) для сдвига кластеров к дорожной сети. Ответ на запрос \emph{route} содержит список точек, через которые проложен маршрут, и список маршрутов; каждый маршрут содержит свою длину. Ответ на запрос \emph{nearest} содержит список точек на графе дорог с указанием расстояний до них.

Для работы движок маршрутизации должен распаковать карту сервиса \emph{Open Street Map}~\cite{OSM} для выделения на ней графа дорог. Главным параметром при работе с движком маршрутизации является используемый профиль (англ. \emph{profile}). При сборке OSRM из исходных кодов предоставляется несколько профилей, среди который профили автомобиля, велосипеда и пешехода. Используемый профиль влияет на способ построения маршрута и выбор городских дород для его прокладки. При работе с метрикой \emph{Route} использовался профиль автомобиля. Пример работы сервиса Open Source Routing Machine представлен на рисунке~\ref{pic:osrm}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.85\textwidth]{osrm}\\[1ex]
    \parbox{.9\textwidth}{\caption{Пример работы сервиса Open Source Routing Machine~\cite{OSRM}}\label{pic:osrm}}
\end{figure}

\section{Заключение}
В данной главе были рассмотрены алгоритмы кластеризации, которые можно использовать при внедрении соответствующих метрик и точной настройке параметров для кластеризации геораспределенных данных: алгоритмы Mean Shift, DBSCAN и BIRCH. Также были описаны основной алгоритм кластеризации, используемый в работе,~--- алгоритм k-means; и метрики, ипользуемые для расчета расстояния между объектами входной выборки,~--- \emph{Route} и \emph{Surface}.
